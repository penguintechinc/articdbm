"""
PyDAL Database Models for ArticDBM Big Data Infrastructure

Defines all database tables for big data clusters, job management, and storage
backends using PyDAL ORM with proper validation, relationships, and timestamp tracking.
"""

from datetime import datetime
from pydal import DAL, Field
from pydal.validators import IS_IN_SET


def define_bigdata_models(db: DAL) -> None:
    """
    Define all PyDAL models for big data infrastructure in ArticDBM.

    Args:
        db: PyDAL database instance
    """

    # ========================
    # HDFS Clusters
    # ========================

    db.define_table(
        'hdfs_clusters',
        Field('name', 'string', required=True, unique=True),
        Field('description', 'text'),
        Field('provider_id', 'reference providers', required=True),
        Field('application_id', 'reference applications', required=True),
        Field('state', 'string', default='pending',
              requires=IS_IN_SET(['pending', 'creating', 'running', 'scaling',
                                 'stopped', 'terminating', 'terminated', 'error']),
              comment='Cluster lifecycle state'),
        Field('cluster_mode', 'string', required=True,
              requires=IS_IN_SET(['standalone', 'yarn', 'kubernetes', 'managed']),
              comment='HDFS deployment mode'),
        Field('namenode_endpoint', 'string', required=True,
              comment='HDFS NameNode endpoint (hostname:port)'),
        Field('namenode_port', 'integer', default=9000,
              comment='HDFS NameNode RPC port'),
        Field('http_port', 'integer', default=50070,
              comment='NameNode HTTP web UI port'),
        Field('datanode_count', 'integer', default=3,
              comment='Number of DataNodes'),
        Field('replication_factor', 'integer', default=3,
              comment='Default HDFS replication factor'),
        Field('block_size_mb', 'integer', default=256,
              comment='HDFS block size in MB'),
        Field('version', 'string',
              comment='Hadoop/HDFS version'),
        Field('configuration', 'json', default='{}',
              comment='HDFS-specific configuration'),
        Field('metadata', 'json', default='{}',
              comment='Additional cluster metadata'),
        Field('status_message', 'text',
              comment='Status detail message'),
        Field('tags', 'json', default='{}',
              comment='Cluster tags for categorization'),
        Field('is_active', 'boolean', default=True),
        Field('created_by', 'reference auth_user'),
        Field('created_on', 'datetime', default=datetime.utcnow, writable=False),
        Field('modified_on', 'datetime', update=datetime.utcnow, writable=False),
        format='%(name)s',
    )

    # ========================
    # Trino Clusters
    # ========================

    db.define_table(
        'trino_clusters',
        Field('name', 'string', required=True, unique=True),
        Field('description', 'text'),
        Field('provider_id', 'reference providers', required=True),
        Field('application_id', 'reference applications', required=True),
        Field('state', 'string', default='pending',
              requires=IS_IN_SET(['pending', 'creating', 'running', 'scaling',
                                 'stopped', 'terminating', 'terminated', 'error']),
              comment='Cluster lifecycle state'),
        Field('cluster_mode', 'string', required=True,
              requires=IS_IN_SET(['standalone', 'yarn', 'kubernetes', 'managed']),
              comment='Trino deployment mode'),
        Field('coordinator_endpoint', 'string', required=True,
              comment='Trino coordinator endpoint (hostname:port)'),
        Field('coordinator_port', 'integer', default=8080,
              comment='Trino coordinator HTTP port'),
        Field('worker_count', 'integer', default=3,
              comment='Number of Trino workers'),
        Field('version', 'string',
              comment='Trino version'),
        Field('memory_gb', 'integer',
              comment='Total memory allocation in GB'),
        Field('max_query_memory_gb', 'integer',
              comment='Max query memory in GB'),
        Field('spill_enabled', 'boolean', default=True,
              comment='Enable spill to disk for large queries'),
        Field('configuration', 'json', default='{}',
              comment='Trino-specific configuration'),
        Field('metadata', 'json', default='{}',
              comment='Additional cluster metadata'),
        Field('status_message', 'text',
              comment='Status detail message'),
        Field('tags', 'json', default='{}',
              comment='Cluster tags for categorization'),
        Field('is_active', 'boolean', default=True),
        Field('created_by', 'reference auth_user'),
        Field('created_on', 'datetime', default=datetime.utcnow, writable=False),
        Field('modified_on', 'datetime', update=datetime.utcnow, writable=False),
        format='%(name)s',
    )

    # ========================
    # Spark Clusters
    # ========================

    db.define_table(
        'spark_clusters',
        Field('name', 'string', required=True, unique=True),
        Field('description', 'text'),
        Field('provider_id', 'reference providers', required=True),
        Field('application_id', 'reference applications', required=True),
        Field('state', 'string', default='pending',
              requires=IS_IN_SET(['pending', 'creating', 'running', 'scaling',
                                 'stopped', 'terminating', 'terminated', 'error']),
              comment='Cluster lifecycle state'),
        Field('cluster_mode', 'string', required=True,
              requires=IS_IN_SET(['standalone', 'yarn', 'kubernetes', 'managed']),
              comment='Spark deployment mode (Standalone, YARN, K8s, Managed)'),
        Field('master_endpoint', 'string', required=True,
              comment='Spark Master endpoint (hostname:port)'),
        Field('master_port', 'integer', default=7077,
              comment='Spark Master communication port'),
        Field('master_web_ui_port', 'integer', default=8080,
              comment='Spark Master web UI port'),
        Field('worker_count', 'integer', default=3,
              comment='Number of Spark worker nodes'),
        Field('executor_cores', 'integer', default=4,
              comment='Cores per executor'),
        Field('executor_memory_gb', 'integer', default=8,
              comment='Memory per executor in GB'),
        Field('driver_memory_gb', 'integer', default=4,
              comment='Driver memory in GB'),
        Field('version', 'string',
              comment='Apache Spark version'),
        Field('hadoop_version', 'string',
              comment='Hadoop version (for Spark integration)'),
        Field('python_version', 'string', default='3.9',
              comment='Python version for PySpark'),
        Field('configuration', 'json', default='{}',
              comment='Spark-specific configuration'),
        Field('metadata', 'json', default='{}',
              comment='Additional cluster metadata'),
        Field('status_message', 'text',
              comment='Status detail message'),
        Field('tags', 'json', default='{}',
              comment='Cluster tags for categorization'),
        Field('is_active', 'boolean', default=True),
        Field('created_by', 'reference auth_user'),
        Field('created_on', 'datetime', default=datetime.utcnow, writable=False),
        Field('modified_on', 'datetime', update=datetime.utcnow, writable=False),
        format='%(name)s',
    )

    # ========================
    # Flink Clusters
    # ========================

    db.define_table(
        'flink_clusters',
        Field('name', 'string', required=True, unique=True),
        Field('description', 'text'),
        Field('provider_id', 'reference providers', required=True),
        Field('application_id', 'reference applications', required=True),
        Field('state', 'string', default='pending',
              requires=IS_IN_SET(['pending', 'creating', 'running', 'scaling',
                                 'stopped', 'terminating', 'terminated', 'error']),
              comment='Cluster lifecycle state'),
        Field('cluster_mode', 'string', required=True,
              requires=IS_IN_SET(['standalone', 'yarn', 'kubernetes', 'managed']),
              comment='Flink deployment mode'),
        Field('jobmanager_endpoint', 'string', required=True,
              comment='Flink JobManager endpoint (hostname:port)'),
        Field('jobmanager_port', 'integer', default=6123,
              comment='Flink JobManager RPC port'),
        Field('jobmanager_web_ui_port', 'integer', default=8081,
              comment='Flink JobManager web UI port'),
        Field('taskmanager_count', 'integer', default=3,
              comment='Number of Flink TaskManagers'),
        Field('taskmanager_slots', 'integer', default=4,
              comment='Task slots per TaskManager'),
        Field('taskmanager_memory_gb', 'integer', default=8,
              comment='TaskManager memory in GB'),
        Field('version', 'string',
              comment='Apache Flink version'),
        Field('configuration', 'json', default='{}',
              comment='Flink-specific configuration'),
        Field('metadata', 'json', default='{}',
              comment='Additional cluster metadata'),
        Field('status_message', 'text',
              comment='Status detail message'),
        Field('tags', 'json', default='{}',
              comment='Cluster tags for categorization'),
        Field('is_active', 'boolean', default=True),
        Field('created_by', 'reference auth_user'),
        Field('created_on', 'datetime', default=datetime.utcnow, writable=False),
        Field('modified_on', 'datetime', update=datetime.utcnow, writable=False),
        format='%(name)s',
    )

    # ========================
    # HBase Clusters
    # ========================

    db.define_table(
        'hbase_clusters',
        Field('name', 'string', required=True, unique=True),
        Field('description', 'text'),
        Field('provider_id', 'reference providers', required=True),
        Field('application_id', 'reference applications', required=True),
        Field('state', 'string', default='pending',
              requires=IS_IN_SET(['pending', 'creating', 'running', 'scaling',
                                 'stopped', 'terminating', 'terminated', 'error']),
              comment='Cluster lifecycle state'),
        Field('cluster_mode', 'string', required=True,
              requires=IS_IN_SET(['standalone', 'yarn', 'kubernetes', 'managed']),
              comment='HBase deployment mode'),
        Field('zookeeper_quorum', 'string', required=True,
              comment='ZooKeeper quorum endpoints (host1:2181,host2:2181,...)'),
        Field('hmaster_endpoint', 'string', required=True,
              comment='HMaster endpoint (hostname:port)'),
        Field('hmaster_port', 'integer', default=16010,
              comment='HMaster web UI port'),
        Field('regionserver_count', 'integer', default=3,
              comment='Number of RegionServers'),
        Field('version', 'string',
              comment='Apache HBase version'),
        Field('hdfs_cluster_id', 'reference hdfs_clusters',
              comment='Associated HDFS cluster for storage'),
        Field('configuration', 'json', default='{}',
              comment='HBase-specific configuration'),
        Field('metadata', 'json', default='{}',
              comment='Additional cluster metadata'),
        Field('status_message', 'text',
              comment='Status detail message'),
        Field('tags', 'json', default='{}',
              comment='Cluster tags for categorization'),
        Field('is_active', 'boolean', default=True),
        Field('created_by', 'reference auth_user'),
        Field('created_on', 'datetime', default=datetime.utcnow, writable=False),
        Field('modified_on', 'datetime', update=datetime.utcnow, writable=False),
        format='%(name)s',
    )

    # ========================
    # Storage Backends
    # ========================

    db.define_table(
        'storage_backends',
        Field('name', 'string', required=True, unique=True),
        Field('description', 'text'),
        Field('provider_id', 'reference providers', required=True),
        Field('application_id', 'reference applications', required=True),
        Field('storage_type', 'string', required=True,
              requires=IS_IN_SET(['s3', 'gcs', 'azure_blob', 'minio', 'hdfs']),
              comment='Type of storage backend'),
        Field('endpoint', 'string', required=True,
              comment='Storage endpoint (bucket name, GCS path, Azure container, etc)'),
        Field('region', 'string',
              comment='Cloud region (AWS, GCP, Azure)'),
        Field('access_key_encrypted', 'blob',
              comment='Encrypted access key or API key'),
        Field('secret_key_encrypted', 'blob',
              comment='Encrypted secret key or API secret'),
        Field('bucket_name', 'string', required=True,
              comment='Bucket/container name for storage'),
        Field('path_prefix', 'string',
              comment='Optional path prefix within bucket'),
        Field('kms_key_id', 'string',
              comment='KMS key ID for server-side encryption'),
        Field('versioning_enabled', 'boolean', default=False,
              comment='Enable object versioning'),
        Field('lifecycle_policy', 'json', default='{}',
              comment='Lifecycle policy for automatic transitions'),
        Field('configuration', 'json', default='{}',
              comment='Storage-specific configuration'),
        Field('metadata', 'json', default='{}',
              comment='Additional backend metadata'),
        Field('status', 'string', default='pending',
              requires=IS_IN_SET(['pending', 'creating', 'available', 'modifying',
                                 'deleting', 'deleted', 'failed']),
              comment='Backend status'),
        Field('status_message', 'text',
              comment='Status detail message'),
        Field('tags', 'json', default='{}',
              comment='Backend tags for categorization'),
        Field('is_active', 'boolean', default=True),
        Field('created_by', 'reference auth_user'),
        Field('created_on', 'datetime', default=datetime.utcnow, writable=False),
        Field('modified_on', 'datetime', update=datetime.utcnow, writable=False),
        format='%(name)s',
    )

    # ========================
    # Iceberg Catalogs
    # ========================

    db.define_table(
        'iceberg_catalogs',
        Field('name', 'string', required=True, unique=True),
        Field('description', 'text'),
        Field('provider_id', 'reference providers', required=True),
        Field('application_id', 'reference applications', required=True),
        Field('catalog_type', 'string', required=True,
              requires=IS_IN_SET(['hive', 'glue', 'rest', 'jdbc', 'nessie']),
              comment='Iceberg catalog implementation type'),
        Field('storage_backend_id', 'reference storage_backends', required=True,
              comment='Associated object storage backend'),
        # Hive metastore config
        Field('hive_metastore_uri', 'string',
              comment='Hive metastore URI (for hive catalog)'),
        # REST catalog config
        Field('rest_endpoint', 'string',
              comment='REST catalog endpoint URL'),
        Field('rest_credential_type', 'string',
              requires=IS_IN_SET(['none', 'oauth2', 'basic', 'custom']),
              comment='REST catalog authentication type'),
        Field('rest_credential_encrypted', 'blob',
              comment='Encrypted REST credential'),
        # JDBC config
        Field('jdbc_uri', 'string',
              comment='JDBC connection URI'),
        Field('jdbc_username', 'string',
              comment='JDBC username'),
        Field('jdbc_password_encrypted', 'blob',
              comment='Encrypted JDBC password'),
        # Nessie config
        Field('nessie_endpoint', 'string',
              comment='Nessie server endpoint'),
        Field('nessie_branch', 'string', default='main',
              comment='Default Nessie branch'),
        Field('nessie_auth_token_encrypted', 'blob',
              comment='Encrypted Nessie auth token'),
        # General config
        Field('warehouse_location', 'string', required=True,
              comment='Root warehouse location on storage backend'),
        Field('configuration', 'json', default='{}',
              comment='Catalog-specific configuration'),
        Field('metadata', 'json', default='{}',
              comment='Additional catalog metadata'),
        Field('status', 'string', default='pending',
              requires=IS_IN_SET(['pending', 'creating', 'available', 'modifying',
                                 'deleting', 'deleted', 'failed']),
              comment='Catalog status'),
        Field('status_message', 'text',
              comment='Status detail message'),
        Field('tags', 'json', default='{}',
              comment='Catalog tags for categorization'),
        Field('is_active', 'boolean', default=True),
        Field('created_by', 'reference auth_user'),
        Field('created_on', 'datetime', default=datetime.utcnow, writable=False),
        Field('modified_on', 'datetime', update=datetime.utcnow, writable=False),
        format='%(name)s',
    )

    # ========================
    # Trino Catalogs
    # ========================

    db.define_table(
        'trino_catalogs',
        Field('name', 'string', required=True, unique=True),
        Field('description', 'text'),
        Field('provider_id', 'reference providers', required=True),
        Field('application_id', 'reference applications', required=True),
        Field('trino_cluster_id', 'reference trino_clusters', required=True,
              comment='Associated Trino cluster'),
        Field('connector_type', 'string', required=True,
              requires=IS_IN_SET(['hive', 'iceberg', 'delta_lake', 'postgresql',
                                 'mysql', 'mongodb', 'elasticsearch', 'redis',
                                 'kafka', 's3', 'gcs']),
              comment='Trino connector type'),
        # Hive/Iceberg connector config
        Field('metastore_uri', 'string',
              comment='Hive metastore URI'),
        Field('iceberg_catalog_id', 'reference iceberg_catalogs',
              comment='Associated Iceberg catalog'),
        # Database connectors (PostgreSQL, MySQL, MongoDB)
        Field('connection_endpoint', 'string',
              comment='Connection endpoint (hostname:port)'),
        Field('connection_database', 'string',
              comment='Database/schema name'),
        Field('connection_username', 'string',
              comment='Connection username'),
        Field('connection_password_encrypted', 'blob',
              comment='Encrypted connection password'),
        # Storage connectors (S3, GCS)
        Field('storage_backend_id', 'reference storage_backends',
              comment='Associated storage backend'),
        Field('storage_schema_location', 'string',
              comment='Schema location on storage'),
        # Kafka connector
        Field('kafka_bootstrap_servers', 'string',
              comment='Kafka bootstrap servers'),
        Field('kafka_security_protocol', 'string',
              requires=IS_IN_SET(['plaintext', 'ssl', 'sasl_plaintext', 'sasl_ssl']),
              comment='Kafka security protocol'),
        # General
        Field('configuration', 'json', default='{}',
              comment='Catalog-specific configuration'),
        Field('metadata', 'json', default='{}',
              comment='Additional catalog metadata'),
        Field('status', 'string', default='pending',
              requires=IS_IN_SET(['pending', 'creating', 'available', 'modifying',
                                 'deleting', 'deleted', 'failed']),
              comment='Catalog status'),
        Field('status_message', 'text',
              comment='Status detail message'),
        Field('tags', 'json', default='{}',
              comment='Catalog tags for categorization'),
        Field('is_active', 'boolean', default=True),
        Field('created_by', 'reference auth_user'),
        Field('created_on', 'datetime', default=datetime.utcnow, writable=False),
        Field('modified_on', 'datetime', update=datetime.utcnow, writable=False),
        format='%(name)s',
    )

    # ========================
    # Spark Jobs
    # ========================

    db.define_table(
        'spark_jobs',
        Field('name', 'string', required=True),
        Field('description', 'text'),
        Field('spark_cluster_id', 'reference spark_clusters', required=True,
              comment='Associated Spark cluster'),
        Field('application_id', 'reference applications', required=True),
        Field('job_type', 'string', required=True,
              requires=IS_IN_SET(['batch', 'streaming']),
              comment='Spark job type'),
        Field('state', 'string', default='pending',
              requires=IS_IN_SET(['pending', 'running', 'completed', 'failed', 'cancelled']),
              comment='Job execution state'),
        Field('jar_file', 'string', required=True,
              comment='Path to JAR file or Python script'),
        Field('main_class', 'string',
              comment='Main class for JAR jobs'),
        Field('arguments', 'json', default='[]',
              comment='Job arguments as JSON array'),
        Field('spark_config', 'json', default='{}',
              comment='Spark configuration properties'),
        Field('executor_cores', 'integer',
              comment='Override executor cores'),
        Field('executor_memory_gb', 'integer',
              comment='Override executor memory in GB'),
        Field('driver_cores', 'integer',
              comment='Override driver cores'),
        Field('driver_memory_gb', 'integer',
              comment='Override driver memory in GB'),
        Field('max_retries', 'integer', default=0,
              comment='Maximum retry attempts'),
        Field('timeout_minutes', 'integer',
              comment='Job timeout in minutes'),
        Field('priority', 'string', default='normal',
              requires=IS_IN_SET(['low', 'normal', 'high']),
              comment='Job priority'),
        Field('submitted_at', 'datetime',
              comment='Job submission timestamp'),
        Field('started_at', 'datetime',
              comment='Job start timestamp'),
        Field('completed_at', 'datetime',
              comment='Job completion timestamp'),
        Field('duration_seconds', 'integer',
              comment='Total job duration in seconds'),
        Field('result', 'json', default='{}',
              comment='Job result and output'),
        Field('error_message', 'text',
              comment='Error message if job failed'),
        Field('metadata', 'json', default='{}',
              comment='Additional job metadata'),
        Field('tags', 'json', default='{}',
              comment='Job tags for categorization'),
        Field('created_by', 'reference auth_user'),
        Field('created_on', 'datetime', default=datetime.utcnow, writable=False),
        Field('modified_on', 'datetime', update=datetime.utcnow, writable=False),
        format='%(name)s',
    )

    # ========================
    # Flink Jobs
    # ========================

    db.define_table(
        'flink_jobs',
        Field('name', 'string', required=True),
        Field('description', 'text'),
        Field('flink_cluster_id', 'reference flink_clusters', required=True,
              comment='Associated Flink cluster'),
        Field('application_id', 'reference applications', required=True),
        Field('job_type', 'string', required=True,
              requires=IS_IN_SET(['batch', 'streaming']),
              comment='Flink job type'),
        Field('state', 'string', default='pending',
              requires=IS_IN_SET(['pending', 'running', 'completed', 'failed', 'cancelled']),
              comment='Job execution state'),
        Field('jar_file', 'string', required=True,
              comment='Path to JAR file'),
        Field('main_class', 'string', required=True,
              comment='Main class for Flink job'),
        Field('arguments', 'json', default='[]',
              comment='Job arguments as JSON array'),
        Field('flink_config', 'json', default='{}',
              comment='Flink configuration properties'),
        Field('parallelism', 'integer', default=1,
              comment='Job parallelism level'),
        Field('max_task_failures', 'integer', default=3,
              comment='Maximum task failures before job fails'),
        Field('timeout_minutes', 'integer',
              comment='Job timeout in minutes'),
        Field('checkpoint_interval_seconds', 'integer',
              comment='Checkpoint interval for streaming jobs'),
        Field('checkpoint_mode', 'string',
              requires=IS_IN_SET(['at_least_once', 'exactly_once']),
              comment='Checkpoint guarantee mode'),
        Field('priority', 'string', default='normal',
              requires=IS_IN_SET(['low', 'normal', 'high']),
              comment='Job priority'),
        Field('submitted_at', 'datetime',
              comment='Job submission timestamp'),
        Field('started_at', 'datetime',
              comment='Job start timestamp'),
        Field('completed_at', 'datetime',
              comment='Job completion timestamp'),
        Field('duration_seconds', 'integer',
              comment='Total job duration in seconds'),
        Field('result', 'json', default='{}',
              comment='Job result and output'),
        Field('error_message', 'text',
              comment='Error message if job failed'),
        Field('metrics', 'json', default='{}',
              comment='Job metrics (records processed, throughput, etc)'),
        Field('metadata', 'json', default='{}',
              comment='Additional job metadata'),
        Field('tags', 'json', default='{}',
              comment='Job tags for categorization'),
        Field('created_by', 'reference auth_user'),
        Field('created_on', 'datetime', default=datetime.utcnow, writable=False),
        Field('modified_on', 'datetime', update=datetime.utcnow, writable=False),
        format='%(name)s',
    )

    db.commit()
